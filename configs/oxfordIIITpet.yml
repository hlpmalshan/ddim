data:
    dataset: "OXFORD_IIIT_PET"  # Dataset name
    image_size: 64              # Image size for resizing
    channels: 3                 # Number of image channels (RGB)
    logit_transform: false      # Apply logit transform
    uniform_dequantization: false  # Enable uniform dequantization
    gaussian_dequantization: false # Enable Gaussian dequantization
    random_flip: true           # Enable random horizontal flip
    rescaled: true              # Rescale images to [-1, 1]
    num_workers: 8              # Number of data loader workers

model:
    type: "iso"              # Model type
    in_channels: 3              # Input channels
    out_ch: 3                   # Output channels
    ch: 64                      # Base channel size
    ch_mult: [1, 2, 4, 8]       # Channel multiplier for each layer
    num_res_blocks: 2           # Number of residual blocks
    attn_resolutions: []   # Resolutions with attention
    dropout: 0.1                # Dropout rate
    var_type: fixedlarge        # Variance type
    ema_rate: 0.999             # EMA decay rate
    ema: True                   # Enable Exponential Moving Average
    resamp_with_conv: True      # Use convolutional resampling
    save_statistics: True       # if True: save batch mean and standard deviation in .csv 

diffusion:
    beta_schedule: linear       # Beta schedule for diffusion
    beta_start: 0.0001          # Starting beta value
    beta_end: 0.02              # Ending beta value
    num_diffusion_timesteps: 1000  # Number of diffusion timesteps

training:
    batch_size: 8              # Training batch size
    n_epochs: 1090              # Number of training epochs
    n_iters: 501400            # Total training iterations
    snapshot_freq: 25070           # Snapshot frequency
    #validation_freq: 2000       # Validation frequency

sampling:
    batch_size: 8              # Sampling batch size
    last_only: True             # Only return the last timestep samples
    ckpt_id: 501400

optim:
    weight_decay: 0.000         # Weight decay for optimizer
    optimizer: "Adam"           # Optimizer type
    lr: 0.00005                  # Learning rate
    beta1: 0.9                  # Beta1 for Adam
    amsgrad: false              # Use AMSGrad variant
    eps: 1.0e-8                   # Epsilon for numerical stability
    grad_clip: 1.0              # Gradient clipping value
